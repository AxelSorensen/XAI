{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model \"ConceptModel\" (image(x) -> concept(c)) could just be a pretrained resnet. It should take an image as input and output a vector of size 112 representing the concepts (binary attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConceptModel, self).__init__()\n",
    "        # Pre-trained ResNet50\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 312) #Updated last layer to 112\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.sigmoid(x)  # Sigmoid for probabilities of concept?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part of the model \"PredictionModel\" (concepts(c) -> prediction(y)) should take the output vector from the conceptmodel in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(312, 256)  # Concept vector as input in the first layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 200)  # Output layer for 200 bird species\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, c):\n",
    "        c = self.relu(self.fc1(c))\n",
    "        c = self.softmax(self.fc2(c))\n",
    "        #c=self.fc2(c)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_dir = f'{os.getcwd()}/CUB_200_2011/images/'\n",
    "root_dir = f'{os.getcwd()}\\\\CUB_200_2011\\\\images\\\\'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Resize((299, 299)),  # Resize images to a fixed size, for example, 224x224\n",
    "    ToTensor()           # Convert images to tensors\n",
    "])\n",
    "dataset = ImageFolder(root=root_dir,transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 9424\n",
      "Validation set size: 1178\n",
      "Test set size: 1179\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = int(0.1 * len(dataset))    # 10% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Split the dataset randomly into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "\n",
    "def extract_data(data_dir):\n",
    "    cwd = os.getcwd()\n",
    "    data_path = join(cwd, data_dir + \"\\\\images\")\n",
    "    val_ratio = 0.1\n",
    "\n",
    "    path_to_id_map = dict()  # map from full image path to image id\n",
    "    with open(data_path.replace(\"images\", \"images.txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            path_to_id_map[join(data_path, items[1])] = int(items[0])\n",
    "\n",
    "    attribute_labels_all = ddict(\n",
    "        list\n",
    "    )  # map from image id to a list of attribute labels\n",
    "    attribute_certainties_all = ddict(\n",
    "        list\n",
    "    )  # map from image id to a list of attribute certainties\n",
    "    attribute_uncertain_labels_all = ddict(\n",
    "        list\n",
    "    )  # map from image id to a list of attribute labels calibrated for uncertainty\n",
    "    # 1 = not visible, 2 = guessing, 3 = probably, 4 = definitely\n",
    "    uncertainty_map = {\n",
    "        1: {\n",
    "            1: 0,\n",
    "            2: 0.5,\n",
    "            3: 0.75,\n",
    "            4: 1,\n",
    "        },  # calibrate main label based on uncertainty label\n",
    "        0: {1: 0, 2: 0.5, 3: 0.25, 4: 0},\n",
    "    }\n",
    "    with open(join(cwd, data_dir + \"\\\\attributes\\\\image_attribute_labels.txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            file_idx, attribute_idx, attribute_label, attribute_certainty = (\n",
    "                line.strip().split()[:4]\n",
    "            )\n",
    "            attribute_label = int(attribute_label)\n",
    "            attribute_certainty = int(attribute_certainty)\n",
    "            uncertain_label = uncertainty_map[attribute_label][attribute_certainty]\n",
    "            attribute_labels_all[int(file_idx)].append(attribute_label)\n",
    "            attribute_uncertain_labels_all[int(file_idx)].append(uncertain_label)\n",
    "            attribute_certainties_all[int(file_idx)].append(attribute_certainty)\n",
    "\n",
    "    is_train_test = dict()  # map from image id to 0 / 1 (1 = train)\n",
    "    with open(join(cwd, data_dir + \"\\\\train_test_split.txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            idx, is_train = line.strip().split()\n",
    "            is_train_test[int(idx)] = int(is_train)\n",
    "    print(\n",
    "        \"Number of train images from official train test split:\",\n",
    "        sum(list(is_train_test.values())),\n",
    "    )\n",
    "    train_val_data, test_data = [], []\n",
    "    train_data, val_data = [], []\n",
    "    folder_list = [f for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    folder_list.sort()  # sort by class index\n",
    "    for i, folder in enumerate(folder_list[:2]):\n",
    "        folder_path = join(data_path, folder)\n",
    "        classfile_list = [\n",
    "            cf\n",
    "            for cf in listdir(folder_path)\n",
    "            if (isfile(join(folder_path, cf)) and cf[0] != \".\")\n",
    "        ]\n",
    "        # classfile_list.sort()\n",
    "        for cf in classfile_list:\n",
    "            img_id = path_to_id_map[join(folder_path, cf)]\n",
    "            img_path = join(folder_path, cf)\n",
    "            metadata = {\n",
    "                \"id\": img_id,\n",
    "                \"img_path\": img_path,\n",
    "                \"img\": dataset[i],\n",
    "                \"class_label\": i,\n",
    "                \"attribute_label\": torch.tensor(attribute_labels_all[img_id],dtype=torch.float32),\n",
    "                \"attribute_certainty\": attribute_certainties_all[img_id],\n",
    "                \"uncertain_attribute_label\": attribute_uncertain_labels_all[img_id],\n",
    "            }\n",
    "            if is_train_test[img_id]:\n",
    "                train_val_data.append(metadata)\n",
    "                # if val_files is not None:\n",
    "                #     if img_path in val_files:\n",
    "                #         val_data.append(metadata)\n",
    "                #     else:\n",
    "                #         train_data.append(metadata)\n",
    "            else:\n",
    "                test_data.append(metadata)\n",
    "\n",
    "    random.shuffle(train_val_data)\n",
    "    split = int(val_ratio * len(train_val_data))\n",
    "    train_data = train_val_data[split:]\n",
    "    val_data = train_val_data[:split]\n",
    "    print(\"Size of train set:\", len(train_data))\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images from official train test split: 5994\n",
      "Size of train set: 54\n"
     ]
    }
   ],
   "source": [
    "data_dir = f'{os.getcwd()}\\\\CUB_200_2011'\n",
    "train_dataset, val_dataset, test_dataset = extract_data(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottleneck model (the two combined in one module) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was thinking CrossEntropyLoss. Since the bottleneck model includes pre-trained components we might want to use different learning rates for different parts of the model? but I think it is maybe possible with PyTorch optimizers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BottleneckModel, self).__init__()\n",
    "        self.concept_model = ConceptModel()\n",
    "        self.prediction_model = PredictionModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        concepts = self.concept_model(x)\n",
    "        predictions = self.prediction_model(concepts)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = BottleneckModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6426,  1.4525, -0.2984,  ...,  0.1987,  0.0388, -0.0450],\n",
      "        [ 1.1664,  1.0266, -0.3414,  ...,  0.1935, -0.0313,  0.0350],\n",
      "        [ 1.1664,  1.0266, -0.3414,  ...,  0.1935, -0.0313,  0.0350],\n",
      "        ...,\n",
      "        [ 0.6426,  1.4525, -0.2984,  ...,  0.1987,  0.0388, -0.0450],\n",
      "        [ 0.6426,  1.4525, -0.2984,  ...,  0.1987,  0.0388, -0.0450],\n",
      "        [ 1.1664,  1.0266, -0.3414,  ...,  0.1935, -0.0313,  0.0350]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 134.8942,  170.9030, -162.7710,  ..., -153.2304, -153.6061,\n",
      "         -151.3617],\n",
      "        [ 139.5346,  159.8393, -161.1977,  ..., -150.7359, -150.1604,\n",
      "         -147.7901],\n",
      "        [ 139.5346,  159.8393, -161.1977,  ..., -150.7359, -150.1604,\n",
      "         -147.7901],\n",
      "        ...,\n",
      "        [ 139.5346,  159.8393, -161.1977,  ..., -150.7359, -150.1604,\n",
      "         -147.7901],\n",
      "        [ 134.8942,  170.9030, -162.7710,  ..., -153.2304, -153.6061,\n",
      "         -151.3617],\n",
      "        [ 134.8942,  170.9030, -162.7710,  ..., -153.2304, -153.6061,\n",
      "         -151.3617]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['img'][0]\n",
    "        labels = batch['class_label']\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)\n",
    "        print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEPENDENT MODEL\n",
    "Independently train the two model. Test the performance on the test_data computing the prediction throug model_pred(model_concep(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8902\n",
      "Epoch 2/5, Loss: 0.7396\n",
      "Epoch 3/5, Loss: 0.7023\n",
      "Epoch 4/5, Loss: 0.6948\n",
      "Epoch 5/5, Loss: 0.6928\n"
     ]
    }
   ],
   "source": [
    "model_c = ConceptModel()\n",
    "#num_attribute_labels = 312\n",
    " # Adjust the number of output classes\n",
    "# Modify the top classification layer\n",
    "#num_ftrs = model.base_model.fc.in_features\n",
    "#model.fc = nn.Linear(num_ftrs, num_attribute_labels)  # Adjust the number of output classes\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_c.base_model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_c.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_c.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['img'][0]\n",
    "        labels = batch['attribute_label']\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_c(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5.2980\n",
      "Epoch 2/20, Loss: 5.2967\n",
      "Epoch 3/20, Loss: 5.2949\n",
      "Epoch 4/20, Loss: 5.2922\n",
      "Epoch 5/20, Loss: 5.2878\n",
      "Epoch 6/20, Loss: 5.2804\n",
      "Epoch 7/20, Loss: 5.2670\n",
      "Epoch 8/20, Loss: 5.2424\n",
      "Epoch 9/20, Loss: 5.1978\n",
      "Epoch 10/20, Loss: 5.1238\n",
      "Epoch 11/20, Loss: 5.0178\n",
      "Epoch 12/20, Loss: 4.8931\n",
      "Epoch 13/20, Loss: 4.7759\n",
      "Epoch 14/20, Loss: 4.6831\n",
      "Epoch 15/20, Loss: 4.6130\n",
      "Epoch 16/20, Loss: 4.5602\n",
      "Epoch 17/20, Loss: 4.5218\n",
      "Epoch 18/20, Loss: 4.4939\n",
      "Epoch 19/20, Loss: 4.4719\n",
      "Epoch 20/20, Loss: 4.4536\n"
     ]
    }
   ],
   "source": [
    "model_p = PredictionModel()\n",
    "\n",
    "optimizer = optim.Adam(model_p.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_p.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_p.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['attribute_label']\n",
    "        labels = batch['class_label']\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_p(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1])\n",
      "tensor([[0.0640, 0.0831, 0.0040,  ..., 0.0038, 0.0054, 0.0037],\n",
      "        [0.0640, 0.0831, 0.0040,  ..., 0.0038, 0.0054, 0.0037],\n",
      "        [0.0640, 0.0831, 0.0040,  ..., 0.0038, 0.0054, 0.0037],\n",
      "        ...,\n",
      "        [0.0640, 0.0831, 0.0040,  ..., 0.0038, 0.0054, 0.0037],\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035],\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "tensor([[0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035],\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035],\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035],\n",
      "        ...,\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035],\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035],\n",
      "        [0.0732, 0.1158, 0.0038,  ..., 0.0037, 0.0052, 0.0035]])\n",
      "557.1206388473511\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_p.eval()\n",
    "model_c.eval()\n",
    "\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    accuracy = 0\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['img'][0]\n",
    "        labels = batch['class_label']\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_p(model_c(inputs))\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    print(running_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02463",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

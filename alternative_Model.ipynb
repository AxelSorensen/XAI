{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model \"ConceptModel\" (image(x) -> concept(c)) could just be a pretrained resnet. It should take an image as input and output a vector of size 112 representing the concepts (binary attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConceptModel, self).__init__()\n",
    "        # Pre-trained ResNet50\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 112) #Updated last layer to 112\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.sigmoid(x)  # Sigmoid for probabilities of concept?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part of the model \"PredictionModel\" (concepts(c) -> prediction(y)) should take the output vector from the conceptmodel in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(112, 256)  # Concept vector as input in the first layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 200)  # Output layer for 200 bird species\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, c):\n",
    "        c = self.relu(self.fc1(c))\n",
    "        c = self.softmax(self.fc2(c))\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottleneck model (the two combined in one module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BottleneckModel, self).__init__()\n",
    "        self.concept_model = ConceptModel()\n",
    "        self.prediction_model = PredictionModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        concepts = self.concept_model(x)\n",
    "        predictions = self.prediction_model(concepts)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert * NICE DATALOADER *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_dir = f'{os.getcwd()}/CUB_200_2011/images/'\n",
    "root_dir = f'{os.getcwd()}\\\\CUB_200_2011\\\\images\\\\'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Resize((299, 299)),  # Resize images to a fixed size, for example, 224x224\n",
    "    ToTensor()           # Convert images to tensors\n",
    "])\n",
    "dataset = ImageFolder(root=root_dir,transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 9424\n",
      "Validation set size: 1178\n",
      "Test set size: 1179\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = int(0.1 * len(dataset))    # 10% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Split the dataset randomly into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "\n",
    "def extract_data(data_dir):\n",
    "    cwd = os.getcwd()\n",
    "    data_path = join(cwd, data_dir + \"\\\\images\")\n",
    "    val_ratio = 0.1\n",
    "\n",
    "    path_to_id_map = dict()  # map from full image path to image id\n",
    "    with open(data_path.replace(\"images\", \"images.txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            path_to_id_map[join(data_path, items[1])] = int(items[0])\n",
    "\n",
    "    attribute_labels_all = ddict(\n",
    "        list\n",
    "    )  # map from image id to a list of attribute labels\n",
    "    attribute_certainties_all = ddict(\n",
    "        list\n",
    "    )  # map from image id to a list of attribute certainties\n",
    "    attribute_uncertain_labels_all = ddict(\n",
    "        list\n",
    "    )  # map from image id to a list of attribute labels calibrated for uncertainty\n",
    "    # 1 = not visible, 2 = guessing, 3 = probably, 4 = definitely\n",
    "    uncertainty_map = {\n",
    "        1: {\n",
    "            1: 0,\n",
    "            2: 0.5,\n",
    "            3: 0.75,\n",
    "            4: 1,\n",
    "        },  # calibrate main label based on uncertainty label\n",
    "        0: {1: 0, 2: 0.5, 3: 0.25, 4: 0},\n",
    "    }\n",
    "    with open(join(cwd, data_dir + \"\\\\attributes\\\\image_attribute_labels.txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            file_idx, attribute_idx, attribute_label, attribute_certainty = (\n",
    "                line.strip().split()[:4]\n",
    "            )\n",
    "            attribute_label = int(attribute_label)\n",
    "            attribute_certainty = int(attribute_certainty)\n",
    "            uncertain_label = uncertainty_map[attribute_label][attribute_certainty]\n",
    "            attribute_labels_all[int(file_idx)].append(attribute_label)\n",
    "            attribute_uncertain_labels_all[int(file_idx)].append(uncertain_label)\n",
    "            attribute_certainties_all[int(file_idx)].append(attribute_certainty)\n",
    "\n",
    "    is_train_test = dict()  # map from image id to 0 / 1 (1 = train)\n",
    "    with open(join(cwd, data_dir + \"\\\\train_test_split.txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            idx, is_train = line.strip().split()\n",
    "            is_train_test[int(idx)] = int(is_train)\n",
    "    print(\n",
    "        \"Number of train images from official train test split:\",\n",
    "        sum(list(is_train_test.values())),\n",
    "    )\n",
    "\n",
    "    train_val_data, test_data = [], []\n",
    "    train_data, val_data = [], []\n",
    "    folder_list = [f for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    folder_list.sort()  # sort by class index\n",
    "    for i, folder in enumerate(folder_list[:2]):\n",
    "        folder_path = join(data_path, folder)\n",
    "        classfile_list = [\n",
    "            cf\n",
    "            for cf in listdir(folder_path)\n",
    "            if (isfile(join(folder_path, cf)) and cf[0] != \".\")\n",
    "        ]\n",
    "        # classfile_list.sort()\n",
    "        for cf in classfile_list:\n",
    "            img_id = path_to_id_map[join(folder_path, cf)]\n",
    "            img_path = join(folder_path, cf)\n",
    "            metadata = {\n",
    "                \"id\": img_id,\n",
    "                \"img_path\": img_path,\n",
    "                \"img\": dataset[i],\n",
    "                \"class_label\": i,\n",
    "                \"attribute_label\": torch.tensor(attribute_labels_all[img_id],dtype=torch.float32),\n",
    "                \"attribute_certainty\": attribute_certainties_all[img_id],\n",
    "                \"uncertain_attribute_label\": attribute_uncertain_labels_all[img_id],\n",
    "            }\n",
    "            if is_train_test[img_id]:\n",
    "                train_val_data.append(metadata)\n",
    "                # if val_files is not None:\n",
    "                #     if img_path in val_files:\n",
    "                #         val_data.append(metadata)\n",
    "                #     else:\n",
    "                #         train_data.append(metadata)\n",
    "            else:\n",
    "                test_data.append(metadata)\n",
    "\n",
    "    random.shuffle(train_val_data)\n",
    "    split = int(val_ratio * len(train_val_data))\n",
    "    train_data = train_val_data[split:]\n",
    "    val_data = train_val_data[:split]\n",
    "    print(\"Size of train set:\", len(train_data))\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images from official train test split: 5994\n",
      "Size of train set: 54\n"
     ]
    }
   ],
   "source": [
    "data_dir = f'{os.getcwd()}\\\\CUB_200_2011'\n",
    "train_dataset, val_dataset, test_dataset = extract_data(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was thinking CrossEntropyLoss. Since the bottleneck model includes pre-trained components we might want to use different learning rates for different parts of the model? but I think it is maybe possible with PyTorch optimizers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BottleneckModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 5.2897\n",
      "Epoch [2/10], Loss: 5.2773\n",
      "Epoch [3/10], Loss: 5.2565\n",
      "Epoch [4/10], Loss: 5.2138\n",
      "Epoch [5/10], Loss: 5.1288\n",
      "Epoch [6/10], Loss: 4.9849\n",
      "Epoch [7/10], Loss: 4.7918\n",
      "Epoch [8/10], Loss: 4.6142\n",
      "Epoch [9/10], Loss: 4.5019\n",
      "Epoch [10/10], Loss: 4.4412\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['img'][0]\n",
    "        labels = batch['class_label']\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02463",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
